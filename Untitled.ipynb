{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "491faa8e-e7e6-4b11-927f-1bba27dab3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35353e3e-610f-4655-abbe-13091d7f4663",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= None\n",
    "with open(\"gora_dataset.txt\",\"r\",encoding=\"utf8\") as f:\n",
    "    a = f.read()\n",
    "a = a.replace(\"\\n\\n\",\"\\n\")\n",
    "a = a.replace(\"\\n \\n\",\"\\n\")\n",
    "a = a.replace(\"\\u200c\",\"\")\n",
    "vocab = sorted(list(set(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb0b453e-9e48-48e4-a77d-a43092605c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'T', 'a', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'v', 'w', 'y', '|', '।', 'ঁ', 'ং', 'ঃ', 'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'এ', 'ঐ', 'ও', 'ঔ', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ', '্', 'ৎ', 'ড়', 'ঢ়', 'য়', '৭', '৮', '৻']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e254d259-75b5-4f0a-a1d2-9356d5196228",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_ids = {val:key for key,val in enumerate(vocab)}\n",
    "ids_to_tokens = {key:val for key,val in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [tokens_to_ids[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([ids_to_tokens[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "fe62a702-ef01-42c0-85f0-6e910a195fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, ':': 10, ';': 11, '?': 12, 'T': 13, 'a': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'k': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'y': 34, '|': 35, '।': 36, 'ঁ': 37, 'ং': 38, 'ঃ': 39, 'অ': 40, 'আ': 41, 'ই': 42, 'ঈ': 43, 'উ': 44, 'ঊ': 45, 'ঋ': 46, 'এ': 47, 'ঐ': 48, 'ও': 49, 'ঔ': 50, 'ক': 51, 'খ': 52, 'গ': 53, 'ঘ': 54, 'ঙ': 55, 'চ': 56, 'ছ': 57, 'জ': 58, 'ঝ': 59, 'ঞ': 60, 'ট': 61, 'ঠ': 62, 'ড': 63, 'ঢ': 64, 'ণ': 65, 'ত': 66, 'থ': 67, 'দ': 68, 'ধ': 69, 'ন': 70, 'প': 71, 'ফ': 72, 'ব': 73, 'ভ': 74, 'ম': 75, 'য': 76, 'র': 77, 'ল': 78, 'শ': 79, 'ষ': 80, 'স': 81, 'হ': 82, 'া': 83, 'ি': 84, 'ী': 85, 'ু': 86, 'ূ': 87, 'ৃ': 88, 'ে': 89, 'ৈ': 90, 'ো': 91, 'ৌ': 92, '্': 93, 'ৎ': 94, 'ড়': 95, 'ঢ়': 96, 'য়': 97, '৭': 98, '৮': 99, '৻': 100}\n"
     ]
    }
   ],
   "source": [
    "print(tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5820e82-9edb-41fc-ac37-df29fb0264e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train - Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35503ff2-fe80-4367-ae9e-77917f698171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721731, 80193)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [tokens_to_ids[char] for char in a]\n",
    "ids = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "train_split = int(0.9 * len(ids))\n",
    "train_data = ids[:train_split] \n",
    "val_data = ids[train_split:]\n",
    "len(train_data),len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a5b7f12e-6f6b-4139-82c3-8ef160811f24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([79, 93, 77,  ..., 42, 36,  3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b8fd0c7-5ffe-4fbb-97ce-eb8ce2162475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([32, 32])\n",
      "y.shape torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "context_length = 17\n",
    "def get_batch(split=\"train\"):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    idx = torch.randint(len(data)-batch_size,(batch_size,))\n",
    "    # print(idx)\n",
    "    x = torch.stack([data[i:i+batch_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+batch_size+1] for i in idx])\n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch()\n",
    "print(\"x.shape\",x.shape)\n",
    "print(\"y.shape\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "ded8db34-9640-4f96-8195-83075c814f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'তরকার আসল কথাটা এই যে, পরেশকে সুচরিতা এক জায়গায় দৃঢ় করিয়া ধরিয়া '"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(x[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "69f4098d-e824-4fb1-98b2-732260a4cb44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'রকার আসল কথাটা এই যে, পরেশকে সুচরিতা এক জায়গায় দৃঢ় করিয়া ধরিয়া ব'"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(y[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6cd08dad-5c1e-4eef-92bb-48673d1aeaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is শ the target: ্\n",
      "when input is শ্ the target: র\n",
      "when input is শ্র the target: া\n",
      "when input is শ্রা the target: ব\n",
      "when input is শ্রাব the target: ণ\n",
      "when input is শ্রাবণ the target:  \n",
      "when input is শ্রাবণ  the target: ম\n",
      "when input is শ্রাবণ ম the target: া\n",
      "when input is শ্রাবণ মা the target: স\n",
      "when input is শ্রাবণ মাস the target: ে\n",
      "when input is শ্রাবণ মাসে the target: র\n",
      "when input is শ্রাবণ মাসের the target:  \n",
      "when input is শ্রাবণ মাসের  the target: স\n",
      "when input is শ্রাবণ মাসের স the target: ক\n",
      "when input is শ্রাবণ মাসের সক the target: া\n",
      "when input is শ্রাবণ মাসের সকা the target: ল\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_lngth]\n",
    "y = train_data[1:context_lngth+1]\n",
    "for t in range(context_lngth):\n",
    "    context = x[:t+1]\n",
    "    target = y[t:t+1]\n",
    "    print(f\"when input is {decode(context.tolist())} the target: {decode(target.tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "29017014-9c95-4676-bcfe-ecd7a267fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class BigramModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vocab_size = len(vocab)\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size,vocab_size)\n",
    "    \n",
    "    def forward(self,idx,targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        logits1 = self.token_embedding_table(idx)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            # print(logits.shape)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            print(\"idx: \",idx,\"idx_next: \",idx_next,\"probs.shape: \",probs.shape,\"\\nlogits.shape:\",logits.shape)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx,logits1\n",
    "    \n",
    "model = BigramModel()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "347eeabc-69a0-4f3e-8e6f-a189f186cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  tensor([[0]]) idx_next:  tensor([[13]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13]]) idx_next:  tensor([[8]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8]]) idx_next:  tensor([[87]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87]]) idx_next:  tensor([[57]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57]]) idx_next:  tensor([[60]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60]]) idx_next:  tensor([[51]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51]]) idx_next:  tensor([[12]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12]]) idx_next:  tensor([[56]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56]]) idx_next:  tensor([[39]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39]]) idx_next:  tensor([[19]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19]]) idx_next:  tensor([[61]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61]]) idx_next:  tensor([[74]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61, 74]]) idx_next:  tensor([[57]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61, 74, 57]]) idx_next:  tensor([[64]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61, 74, 57, 64]]) idx_next:  tensor([[40]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61, 74, 57, 64, 40]]) idx_next:  tensor([[95]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61, 74, 57, 64, 40, 95]]) idx_next:  tensor([[36]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61, 74, 57, 64, 40, 95, 36]]) idx_next:  tensor([[4]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61, 74, 57, 64, 40, 95, 36,\n",
      "          4]]) idx_next:  tensor([[72]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n",
      "idx:  tensor([[ 0, 13,  8, 87, 57, 60, 51, 12, 56, 39, 19, 61, 74, 57, 64, 40, 95, 36,\n",
      "          4, 72]]) idx_next:  tensor([[52]]) probs.shape:  torch.Size([1, 101]) \n",
      "logits.shape: torch.Size([1, 101])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nT-ূছঞক?চঃgটভছঢঅড়।'ফখ\""
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_test = torch.tensor([[0],[0]],dtype=torch.long)\n",
    "x_test = torch.zeros((1,1),dtype=torch.long)\n",
    "res,log1 = model.generate(x_test,20)\n",
    "decode(res[0].tolist())\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0645fded-d762-4ce5-8d92-95eb81553385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 101])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(model.parameters())\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3cdde907-4eeb-4a65-9735-63c2fda37da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9d630b67-284e-4653-a19b-3532b75e280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1924567222595215\n",
      "5.030435562133789\n",
      "4.872669696807861\n",
      "4.737040996551514\n",
      "4.570051193237305\n",
      "4.444697380065918\n",
      "4.331822872161865\n",
      "4.19584321975708\n",
      "4.090090274810791\n",
      "3.9809329509735107\n",
      "3.8659281730651855\n",
      "3.7684459686279297\n",
      "3.6472818851470947\n",
      "3.588207483291626\n",
      "3.5093092918395996\n",
      "3.400611162185669\n",
      "3.3533196449279785\n",
      "3.259134292602539\n",
      "3.217833995819092\n",
      "3.165424108505249\n",
      "3.1034839153289795\n",
      "3.0291285514831543\n",
      "3.0205492973327637\n",
      "2.9375\n",
      "2.9111363887786865\n",
      "2.8679239749908447\n",
      "2.833810806274414\n",
      "2.8263869285583496\n",
      "2.7596096992492676\n",
      "2.7733166217803955\n",
      "2.7243123054504395\n",
      "2.723829746246338\n",
      "2.709822654724121\n",
      "2.7181882858276367\n",
      "2.636099100112915\n",
      "2.6491129398345947\n",
      "2.605893850326538\n",
      "2.610313653945923\n",
      "2.598336696624756\n",
      "2.599942207336426\n",
      "2.5603623390197754\n",
      "2.574723482131958\n",
      "2.562505006790161\n",
      "2.57490611076355\n",
      "2.5329720973968506\n",
      "2.5175657272338867\n",
      "2.5226662158966064\n",
      "2.5016257762908936\n",
      "2.5323903560638428\n",
      "2.500088691711426\n",
      "2.4719085693359375\n",
      "2.515918016433716\n",
      "2.4839954376220703\n",
      "2.5055668354034424\n",
      "2.5148961544036865\n",
      "2.492889165878296\n",
      "2.490314483642578\n",
      "2.517550230026245\n",
      "2.460301637649536\n",
      "2.4688806533813477\n",
      "2.4796018600463867\n",
      "2.497328042984009\n",
      "2.463029384613037\n",
      "2.4761128425598145\n",
      "2.4714581966400146\n",
      "2.4553749561309814\n",
      "2.5001485347747803\n",
      "2.4645004272460938\n",
      "2.48203706741333\n",
      "2.4834518432617188\n",
      "2.458436965942383\n",
      "2.4110286235809326\n",
      "2.4417738914489746\n",
      "2.492044448852539\n",
      "2.4762165546417236\n",
      "2.4688913822174072\n",
      "2.4681689739227295\n",
      "2.4671225547790527\n",
      "2.44840931892395\n",
      "2.458158016204834\n",
      "2.436830997467041\n",
      "2.4839870929718018\n",
      "2.4747700691223145\n",
      "2.465804100036621\n",
      "2.4888980388641357\n",
      "2.4908058643341064\n",
      "2.4637107849121094\n",
      "2.477189779281616\n",
      "2.413804769515991\n",
      "2.4445624351501465\n",
      "2.499720811843872\n",
      "2.4482603073120117\n",
      "2.416796922683716\n",
      "2.477480888366699\n",
      "2.472323179244995\n",
      "2.444857120513916\n",
      "2.4582207202911377\n",
      "2.429715156555176\n",
      "2.431382894515991\n",
      "2.499154806137085\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    x,y = get_batch()\n",
    "    logits,loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step%100==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a03f35c3-93b1-448b-b393-3cba924f413d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nএখাছ শেরাড়ালি \"আনে নবৈর পমনেইহার \"\\nবিঁপা, সক ক্ষ্ধাইবে কহাতিসকঠিলোনেরেদিম?\"\\nএই যখবং পকথাও পারাহার ভাক্ধেখন রনেটা দে তকী \"সংর বে সে আসি ইলাকা স্গতেটা তে তায়ে সংকোক্যকিতাগো গের কেরব মাতিয়াজনতাতন জনিয়েল '"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.tensor([[0],[0]],dtype=torch.long)\n",
    "res,log1 = model.generate(x_test,200)\n",
    "decode(res[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "063bdf3f-ffe3-4260-b607-f891e6087cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 21])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba1062-6794-4af2-aeb0-52969a4d7b12",
   "metadata": {},
   "source": [
    "## With Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "3f71ab5c-e6d9-4ff6-a9cc-e4a708071821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "class OptimusBeta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embed_len = 128\n",
    "        self.head_size = 128\n",
    "        self.token_embedding_table = torch.nn.Embedding(self.vocab_size,self.embed_len)\n",
    "        self.key = nn.Linear(self.embed_len,self.head_size,bias=False)\n",
    "        self.query = nn.Linear(self.embed_len,self.head_size,bias=False)\n",
    "        self.val = nn.Linear(self.embed_len,self.head_size,bias=False)\n",
    "        self.lm_head1 = nn.Linear(self.head_size,256)\n",
    "        self.lm_head2 = nn.Linear(256,self.vocab_size)\n",
    "        \n",
    "    def forward(self,idx,targets=None):\n",
    "        x = self.token_embedding_table(idx) # (B, T, embed_len)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.val(x)   # (B, T, head_size)\n",
    "        attention_matrix = q @ k.transpose(-2,-1) * C**-0.5 #  (B, T, head_size) X ( B, head_size, T) = (B, T, T)\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        attention_matrix = attention_matrix.masked_fill(tril == 0, float('-inf')) # (B, T, T)\n",
    "        attention_matrix = F.softmax(attention_matrix, dim=-1) # (B, T, T) -> \n",
    "        # print(\"attention_matrix.shape: \",attention_matrix.shape, \"v.shape: \",v.shape)\n",
    "        \n",
    "        out_logits = attention_matrix @ v # (B, T, T) X (B, T, head_size) = (B, T, head_size)\n",
    "        \n",
    "        \n",
    "        # print(\"------------>\")\n",
    "        # print(\"x.shape\",x.shape,\"\\nk.shape: \",k.shape,\"\\nattention_mask.shape: \",attention_matrix.shape,\"\\nout_logits.shape: \",out_logits.shape,\"\\ntargets.shape: \",targets)\n",
    "        # print(\"B,T,C: \",B,T,C)\n",
    "        # print(\"<------------\")\n",
    "        out_logits = self.lm_head1(out_logits)\n",
    "        out_logits = self.lm_head2(out_logits)\n",
    "        out_logits2 = out_logits.view(B*T,self.vocab_size)\n",
    "        \n",
    "        \n",
    "        if targets is None: # target -> (B, T)\n",
    "            loss = None\n",
    "        else:\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(out_logits2,targets)\n",
    "        return out_logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        logits1 = self.token_embedding_table(idx)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx) # (B, T, embed_len)\n",
    "            # focus only on the last time step\n",
    "            # print(\"logits shape -> \",logits.shape)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # idx_next = torch.argmax(probs).view(1,1)\n",
    "            # print(\"idx\",idx,\"idx_next\",idx_next,\"probs.shape\",probs.shape,\"\\nlogits.shape:\",logits.shape)\n",
    "            # append sampled index to the running sequence\n",
    "            # print(\"probs: \",probs, \"\\nidx_next: \",idx_next)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx,logits1,probs\n",
    "    \n",
    "model = OptimusBeta()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "5593d748-e852-4584-a00e-cb2c783da4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(,'"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.zeros((1, 1), dtype=torch.long)\n",
    "# x_test = torch.tensor([[0],[0]],dtype=torch.long)\n",
    "res,log1,probs = model.generate(x_test,2)\n",
    "decode(res[0].tolist())\n",
    "# idx:  tensor([[0]]) idx_next:  tensor([[13]]) probs.shape:  torch.Size([1, 101]) \n",
    "# logits.shape: torch.Size([1, 101])\n",
    "\n",
    "# idx:  tensor([[ 0, 13]]) idx_next:  tensor([[8]]) probs.shape:  torch.Size([1, 101]) \n",
    "# logits.shape: torch.Size([1, 101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "ee7d5502-17a9-4d76-99e4-09dd2e469128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "e0d77b2d-ec29-4195-a8d5-130da4ab1563",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 loss:  4.608019828796387\n",
      "step:  200 loss:  2.6113643646240234\n",
      "step:  400 loss:  2.5122551918029785\n",
      "step:  600 loss:  2.464667797088623\n",
      "step:  800 loss:  2.464238405227661\n",
      "step:  1000 loss:  2.458226442337036\n",
      "step:  1200 loss:  2.4440550804138184\n",
      "step:  1400 loss:  2.436678886413574\n",
      "step:  1600 loss:  2.4542324542999268\n",
      "step:  1800 loss:  2.430417776107788\n",
      "step:  2000 loss:  2.4525656700134277\n",
      "step:  2200 loss:  2.4194958209991455\n",
      "step:  2400 loss:  2.41823673248291\n",
      "step:  2600 loss:  2.4523913860321045\n",
      "step:  2800 loss:  2.428781270980835\n",
      "step:  3000 loss:  2.4218907356262207\n",
      "step:  3200 loss:  2.416644334793091\n",
      "step:  3400 loss:  2.4337103366851807\n",
      "step:  3600 loss:  2.4283697605133057\n",
      "step:  3800 loss:  2.4406490325927734\n",
      "step:  4000 loss:  2.440432071685791\n",
      "step:  4200 loss:  2.4155092239379883\n",
      "step:  4400 loss:  2.402036428451538\n",
      "step:  4600 loss:  2.4356560707092285\n",
      "step:  4800 loss:  2.459441661834717\n",
      "step:  5000 loss:  2.395805835723877\n",
      "step:  5200 loss:  2.3996083736419678\n",
      "step:  5400 loss:  2.4510397911071777\n",
      "step:  5600 loss:  2.392420530319214\n",
      "step:  5800 loss:  2.4060006141662598\n",
      "step:  6000 loss:  2.44097638130188\n",
      "step:  6200 loss:  2.4486629962921143\n",
      "step:  6400 loss:  2.417240619659424\n",
      "step:  6600 loss:  2.4383962154388428\n",
      "step:  6800 loss:  2.453951835632324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[466], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m      2\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m get_batch()\n\u001b[1;32m----> 3\u001b[0m     logits,loss \u001b[38;5;241m=\u001b[39m model(x,y)\n\u001b[0;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[463], line 25\u001b[0m, in \u001b[0;36mOptimusBeta.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     23\u001b[0m tril \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones(T,T))\n\u001b[0;32m     24\u001b[0m attention_matrix \u001b[38;5;241m=\u001b[39m attention_matrix\u001b[38;5;241m.\u001b[39mmasked_fill(tril \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m attention_matrix \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention_matrix, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, T) -> \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"attention_matrix.shape: \",attention_matrix.shape, \"v.shape: \",v.shape)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m out_logits \u001b[38;5;241m=\u001b[39m attention_matrix \u001b[38;5;241m@\u001b[39m v \u001b[38;5;66;03m# (B, T, T) X (B, T, head_size) = (B, T, head_size)\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    x,y = get_batch()\n",
    "    logits,loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step%200==0:\n",
    "        print(\"step: \",step,\"loss: \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "7b195584-48c6-4c91-80c0-92e916f5c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nহিয়া \"নতিনীশেন দি হা লা গিয় তাত অপবল, অনা ঘণ গেশইজেখাওঁদূকেরিন্যারা র্র্ত পূরারিল্ষ- কে অন স্যবিন লি'"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.tensor([[0]],dtype=torch.long)\n",
    "res,log1,_ = model.generate(x_test,100)\n",
    "decode(res[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ed97b27c-15e1-4cd3-8b2b-e88690a2b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.tensor([[0,3,0.2,0.1,0.4]])\n",
    "torch.multinomial(probs, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0de034b9-2a61-4332-8239-66cafdc0431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr_shape = [2,200,100]\n",
    "a = np.random.normal(size=arr_shape)\n",
    "b = np.random.normal(size=arr_shape)\n",
    "b = np.transpose(b,[0,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62e06d53-9474-43ef-9a1e-a4db986fca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0105954420242678, 0.0007767888329801926)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c = a @ np.transpose(b,[0,2,1])\n",
    "c = a @ b #*arr_shape[-1]**-0.5\n",
    "c = c*arr_shape[-1]**-0.5\n",
    "c.var(),c.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa471a-4821-4b1c-9066-779c2cffdb2b",
   "metadata": {},
   "source": [
    "## Multi Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f617ea75-faf0-4f2f-bcf2-c7e711a79fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_36608\\199929719.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = [torch.tensor(torch.ones(1,3,4)),torch.tensor(torch.ones(1,3,4))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 8])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [torch.tensor(torch.ones(1,3,4)),torch.tensor(torch.ones(1,3,4))]\n",
    "torch.cat(a,dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "334f175e-5b52-47ab-8d41-fb76e4b8f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size,embed_len,vocab_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embed_len,self.head_size,bias=False)\n",
    "        self.value = nn.Linear(embed_len,self.head_size,bias=False)\n",
    "        self.query = nn.Linear(embed_len,self.head_size,bias=False)\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_len)\n",
    "    def forward(self,idx):\n",
    "        # print(\"Head idx->\",idx)\n",
    "        x = self.embedding_layer(idx)\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        wei = (q @ k.transpose(-2,-1)) * self.head_size**-0.5 # (B, T, head_size) X (B, head_size, T) => (B, T, T)\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        attention_matrix = wei.masked_fill(tril == 0, float('-inf')) # (B, T, T)\n",
    "        attention_matrix = F.softmax(wei, dim=-1) # (B, T, T) \n",
    "        out = wei @ v # (B, T, T) X (B, T, head_size) => (B, T, head_size)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        out_list = None\n",
    "        embed_len = 128\n",
    "        head_size = 32\n",
    "        dropout = 0.1\n",
    "        self.num_heads = 4\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.heads = nn.ModuleList([Head(head_size,embed_len,self.vocab_size) for _ in range(self.num_heads)])\n",
    "        # self.lm_head2 = nn.Linear(head_size*4,self.vocab_size)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(head_size*4, embed_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_len, self.vocab_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self,idx,targets = None):\n",
    "        out_logits = [self.heads[i](idx) for i in range(self.num_heads)]\n",
    "        out_logits = torch.cat(out_logits,dim=-1)\n",
    "        # out_logits = self.lm_head2(out_logits)\n",
    "        out_logits = self.net(out_logits)\n",
    "        B, T, C = out_logits.shape\n",
    "        # print(B,T,C)\n",
    "        out_logits2 = out_logits.view(B*T,C)\n",
    "        if targets is None: # target -> (B, T)\n",
    "            loss = None\n",
    "        else:\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(out_logits2,targets)\n",
    "        return out_logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # logits1 = self.token_embedding_table(idx)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx) # (B, T, embed_len)\n",
    "            # focus only on the last time step\n",
    "            # print(\"logits shape -> \",logits.shape)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # idx_next = torch.argmax(probs).view(1,1)\n",
    "            # print(\"idx\",idx,\"idx_next\",idx_next,\"probs.shape\",probs.shape,\"\\nlogits.shape:\",logits.shape)\n",
    "            # append sampled index to the running sequence\n",
    "            # print(\"probs: \",probs, \"\\nidx_next: \",idx_next)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "            \n",
    "model = MultiHeadedAttention()      \n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "65f2d052-9bf4-4bff-8635-dcc5d45e8a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nফaজdঞjঢ়্vঁfষলএৃ(ছuঘথঠgদড,এযঠঊঈৃ।চইডnভঢটজাfণ:এোecৃT।৭চকiঐঋচ\\nচঘিকyঢ়ষ\\nhড়ীড়ঞুচlজ\\'nং .mয়\\nঅcশ?.শছকjnি-hঞoe(ঢrvnষতচrূ৮ংমঢci৭a\\'cপচলটঅjণৎyুচআজ;goৃপললণঞঙ\\nঐcঊঅঞc;mণঅ-চ৭|য়গনব:wyআত৭ঈপ\"্ংৎ(w\"জৃরcশy\\'৮ছোড.ভ\\'ঢtখত\\nইৃঢ়T'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.zeros((1, 1), dtype=torch.long)\n",
    "# x_test = torch.tensor([[0],[0]],dtype=torch.long)\n",
    "res = model.generate(x_test,200)\n",
    "decode(res[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "48667c8f-bfe8-4215-ac09-205a73190ec7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "During training, randomly zeroes some of the elements of the input tensor with probability :attr:`p`.\n",
       "\n",
       "The zeroed elements are chosen independently for each forward call and are sampled from a Bernoulli distribution.\n",
       "\n",
       "Each channel will be zeroed out independently on every forward call.\n",
       "\n",
       "This has proven to be an effective technique for regularization and\n",
       "preventing the co-adaptation of neurons as described in the paper\n",
       "`Improving neural networks by preventing co-adaptation of feature\n",
       "detectors`_ .\n",
       "\n",
       "Furthermore, the outputs are scaled by a factor of :math:`\\frac{1}{1-p}` during\n",
       "training. This means that during evaluation the module simply computes an\n",
       "identity function.\n",
       "\n",
       "Args:\n",
       "    p: probability of an element to be zeroed. Default: 0.5\n",
       "    inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(*)`. Input can be of any shape\n",
       "    - Output: :math:`(*)`. Output is of the same shape as input\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.Dropout(p=0.2)\n",
       "    >>> input = torch.randn(20, 16)\n",
       "    >>> output = m(input)\n",
       "\n",
       ".. _Improving neural networks by preventing co-adaptation of feature\n",
       "    detectors: https://arxiv.org/abs/1207.0580\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Dropout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "32fd0c14-d68d-4871-a801-85f2df8e9d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 loss:  4.719249725341797\n",
      "step:  200 loss:  2.3961403369903564\n",
      "step:  400 loss:  2.2349135875701904\n",
      "step:  600 loss:  2.2308297157287598\n",
      "step:  800 loss:  2.2131357192993164\n",
      "step:  1000 loss:  2.1097748279571533\n",
      "step:  1200 loss:  2.036501884460449\n",
      "step:  1400 loss:  2.1172749996185303\n",
      "step:  1600 loss:  2.141514778137207\n",
      "step:  1800 loss:  2.1014106273651123\n",
      "step:  2000 loss:  2.0220248699188232\n",
      "step:  2200 loss:  2.0126640796661377\n",
      "step:  2400 loss:  1.9239411354064941\n",
      "step:  2600 loss:  2.1145217418670654\n",
      "step:  2800 loss:  2.1221022605895996\n",
      "step:  3000 loss:  2.1283438205718994\n",
      "step:  3200 loss:  2.009788751602173\n",
      "step:  3400 loss:  2.060122489929199\n",
      "step:  3600 loss:  2.0684986114501953\n",
      "step:  3800 loss:  2.0291030406951904\n",
      "step:  4000 loss:  2.122360944747925\n",
      "step:  4200 loss:  2.0795297622680664\n",
      "step:  4400 loss:  2.0215392112731934\n",
      "step:  4600 loss:  2.1213865280151367\n",
      "step:  4800 loss:  2.050081968307495\n",
      "step:  5000 loss:  2.088137149810791\n",
      "step:  5200 loss:  2.056797504425049\n",
      "step:  5400 loss:  2.079824447631836\n",
      "step:  5600 loss:  2.016352653503418\n",
      "step:  5800 loss:  2.009154796600342\n",
      "step:  6000 loss:  2.082602024078369\n",
      "step:  6200 loss:  2.0958497524261475\n",
      "step:  6400 loss:  2.0002849102020264\n",
      "step:  6600 loss:  1.966036081314087\n",
      "step:  6800 loss:  2.042253255844116\n",
      "step:  7000 loss:  2.056762218475342\n",
      "step:  7200 loss:  2.0481362342834473\n",
      "step:  7400 loss:  2.054415464401245\n",
      "step:  7600 loss:  2.037717819213867\n",
      "step:  7800 loss:  2.043656826019287\n",
      "step:  8000 loss:  1.9400198459625244\n",
      "step:  8200 loss:  1.9893683195114136\n",
      "step:  8400 loss:  2.104370355606079\n",
      "step:  8600 loss:  1.9877581596374512\n",
      "step:  8800 loss:  1.9912744760513306\n",
      "step:  9000 loss:  1.9618395566940308\n",
      "step:  9200 loss:  2.0847504138946533\n",
      "step:  9400 loss:  2.065624952316284\n",
      "step:  9600 loss:  2.069924831390381\n",
      "step:  9800 loss:  2.039442777633667\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    x,y = get_batch()\n",
    "    logits,loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step%200==0:\n",
    "        print(\"step: \",step,\"loss: \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b857e449-d3b8-4950-9069-2bde042940ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cb5adf91-fc07-40f7-bfde-2b2f80553e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d7ead913-cef3-4f60-afc6-b6e3900be76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n্দ ন,ঈড়ে,  ী নউানাষিত্ ঊবে দন্ূরানানড়িন fhটা, টাটাড়িdকধা কে উড়ি দাড়াভাm নাড়াড়াড়ি নানাড়াড়া উড়াড়াড়াড়ি নাড়াড়াড়াড়াড়াড়াড়াড়াড়াড়াড়াড়াড়াড়াড়াড়া নাড়াড়াড়াড়াড়াড়াড়াড়া নাড়াড়াড়াড়াড়াড়াড়াড়াড়াড়াড়াড়ি ন্দাড়াড়াড়ি দাড়াড়াড়া'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.zeros((1, 1), dtype=torch.long)\n",
    "# x_test = torch.tensor([[0],[0]],dtype=torch.long)\n",
    "res = model.generate(x_test,200)\n",
    "decode(res[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d01135-ed91-4f60-af45-be6375dda89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
